{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment_analysis_exam.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3FitnjtxL9k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import pandas as pd \n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt \n",
        "import seaborn as sns\n",
        "import string\n",
        "import nltk\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SgZiKi7xctK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train  = pd.read_csv('train_E6oV3lV.csv')\n",
        "test = pd.read_csv('test_tweets_anuFYb8.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6K1Tpzckxji2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wB8WYVv11TbN",
        "colab_type": "text"
      },
      "source": [
        "we will check the distribution of length of the tweets, in terms of words, in both train and test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgwFuEJly6Zq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "length_train = train['tweet'].str.len()\n",
        "length_test = test['tweet'].str.len()\n",
        "\n",
        "plt.hist(length_train, bins=20, label=\"train_tweets\")\n",
        "plt.hist(length_test, bins=20, label=\"test_tweets\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KofyxaoUxpTm",
        "colab_type": "text"
      },
      "source": [
        "## Removing @user\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOpKRNEMxsdT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "combi = train.append(test, ignore_index=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhJvf08kxveE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_pattern(input_txt, pattern):\n",
        "    r = re.findall(pattern, input_txt)\n",
        "    for i in r:\n",
        "        input_txt = re.sub(i, '', input_txt)\n",
        "        \n",
        "    return input_txt    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6qEdwdUxyWr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "combi['tidy_tweet'] = np.vectorize(remove_pattern)(combi['tweet'], \"@[\\w]*\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0WkHiuKBCLj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hilzkYaqxyYS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "combi['tidy_tweet'] = combi['tidy_tweet'].str.replace(\"[^a-zA-Z#]\", \" \")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSAcFX84xycl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "combi['tidy_tweet'] = combi['tidy_tweet'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oeZxgcwpxvlt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "combi.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6b9YCbvyCqz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenized_tweet = combi['tidy_tweet'].apply(lambda x: x.split())\n",
        "tokenized_tweet.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9zonp80yCsh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.stem.porter import *\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "tokenized_tweet = tokenized_tweet.apply(lambda x: [stemmer.stem(i) for i in x]) # stemming\n",
        "tokenized_tweet.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAf7fMEeyCv0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(tokenized_tweet)):\n",
        "    tokenized_tweet[i] = ' '.join(tokenized_tweet[i])\n",
        "\n",
        "combi['tidy_tweet'] = tokenized_tweet"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dEC-5UWyCxU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_words = ' '.join([text for text in combi['tidy_tweet']])\n",
        "from wordcloud import WordCloud\n",
        "wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words)\n",
        "\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGoEn0U-yO2d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "normal_words =' '.join([text for text in combi['tidy_tweet'][combi['label'] == 0]])\n",
        "\n",
        "wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(normal_words)\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZTjNdYCyO_v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "negative_words = ' '.join([text for text in combi['tidy_tweet'][combi['label'] == 1]])\n",
        "wordcloud = WordCloud(width=800, height=500,\n",
        "random_state=21, max_font_size=110).generate(negative_words)\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8EfY0n0yPJt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def hashtag_extract(x):\n",
        "    hashtags = []\n",
        "    # Loop over the words in the tweet\n",
        "    for i in x:\n",
        "        ht = re.findall(r\"#(\\w+)\", i)\n",
        "        hashtags.append(ht)\n",
        "\n",
        "    return hashtags"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfcdepg5yWGN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# extracting hashtags from non racist/sexist tweets\n",
        "\n",
        "HT_regular = hashtag_extract(combi['tidy_tweet'][combi['label'] == 0])\n",
        "\n",
        "# extracting hashtags from racist/sexist tweets\n",
        "HT_negative = hashtag_extract(combi['tidy_tweet'][combi['label'] == 1])\n",
        "\n",
        "# unnesting list\n",
        "HT_regular = sum(HT_regular,[])\n",
        "HT_negative = sum(HT_negative,[])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kI-olQoPyWHu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = nltk.FreqDist(HT_regular)\n",
        "d = pd.DataFrame({'Hashtag': list(a.keys()),\n",
        "                  'Count': list(a.values())})\n",
        "# selecting top 10 most frequent hashtags     \n",
        "d = d.nlargest(columns=\"Count\", n = 20) \n",
        "plt.figure(figsize=(16,5))\n",
        "ax = sns.barplot(data=d, x= \"Hashtag\", y = \"Count\")\n",
        "ax.set(ylabel = 'Count')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LgomSl3SyWKy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "b = nltk.FreqDist(HT_negative)\n",
        "e = pd.DataFrame({'Hashtag': list(b.keys()), 'Count': list(b.values())})\n",
        "# selecting top 10 most frequent hashtags\n",
        "e = e.nlargest(columns=\"Count\", n = 20)   \n",
        "plt.figure(figsize=(16,5))\n",
        "ax = sns.barplot(data=e, x= \"Hashtag\", y = \"Count\")\n",
        "ax.set(ylabel = 'Count')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIOpYAV6yWMj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "bow_vectorizer = CountVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\n",
        "# bag-of-words feature matrix\n",
        "bow = bow_vectorizer.fit_transform(combi['tidy_tweet'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMFiJvlWyWP7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\n",
        "# TF-IDF feature matrix\n",
        "tfidf = tfidf_vectorizer.fit_transform(combi['tidy_tweet'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlfvfdwjyWUN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "train_bow = bow[:31962,:]\n",
        "test_bow = bow[31962:,:]\n",
        "\n",
        "# splitting data into training and validation set\n",
        "xtrain_bow, xvalid_bow, ytrain, yvalid = train_test_split(train_bow, train['label'], random_state=42, test_size=0.3)\n",
        "\n",
        "lreg = LogisticRegression()\n",
        "lreg.fit(xtrain_bow, ytrain) # training the model\n",
        "\n",
        "prediction = lreg.predict_proba(xvalid_bow) # predicting on the validation set\n",
        "prediction_int = prediction[:,1] >= 0.3 # if prediction is greater than or equal to 0.3 than 1 else 0\n",
        "prediction_int = prediction_int.astype(np.int)\n",
        "\n",
        "f1_score(yvalid, prediction_int) # calculating f1 score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-dMjVIkyWVx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_pred = lreg.predict_proba(test_bow)\n",
        "test_pred_int = test_pred[:,1] >= 0.3\n",
        "test_pred_int = test_pred_int.astype(np.int)\n",
        "test['label'] = test_pred_int\n",
        "submission = test[['id','label']]\n",
        "submission.to_csv('sub_lreg_bow.csv', index=False) # writing data to a CSV file"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBRXJF6syxrf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_tfidf = tfidf[:31962,:]\n",
        "test_tfidf = tfidf[31962:,:]\n",
        "\n",
        "xtrain_tfidf = train_tfidf[ytrain.index]\n",
        "xvalid_tfidf = train_tfidf[yvalid.index]\n",
        "\n",
        "lreg.fit(xtrain_tfidf, ytrain)\n",
        "\n",
        "prediction = lreg.predict_proba(xvalid_tfidf)\n",
        "prediction_int = prediction[:,1] >= 0.3\n",
        "prediction_int = prediction_int.astype(np.int)\n",
        "\n",
        "f1_score(yvalid, prediction_int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ht9xrKAnzXtT",
        "colab_type": "text"
      },
      "source": [
        "## Word Embeddings\n",
        "### 1. Word2Vec Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NvhdKWszWxy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenized_tweet = combi['tidy_tweet'].apply(lambda x: x.split()) # tokenizing\n",
        "\n",
        "model_w2v = gensim.models.Word2Vec(\n",
        "            tokenized_tweet,\n",
        "            size=200, # desired no. of features/independent variables \n",
        "            window=5, # context window size\n",
        "            min_count=2,\n",
        "            sg = 1, # 1 for skip-gram model\n",
        "            hs = 0,\n",
        "            negative = 10, # for negative sampling\n",
        "            workers= 2, # no.of cores\n",
        "            seed = 34)\n",
        "\n",
        "model_w2v.train(tokenized_tweet, total_examples= len(combi['tidy_tweet']), epochs=20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5IuHuxffzh3X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_w2v.wv.most_similar(positive=\"dinner\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJr3ZrLlzh5C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_w2v.wv.most_similar(positive=\"trump\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBEwzMsozh-f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_w2v['food']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clFrmuh5ziDF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(model_w2v['food'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNJreUGmz4Qd",
        "colab_type": "text"
      },
      "source": [
        "Preparing Vectors for **Tweets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIMA3A1Pzh89",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def word_vector(tokens, size):\n",
        "    vec = np.zeros(size).reshape((1, size))\n",
        "    count = 0.\n",
        "    for word in tokens:\n",
        "        try:\n",
        "            vec += model_w2v[word].reshape((1, size))\n",
        "            count += 1.\n",
        "        except KeyError: # handling the case where the token is not in vocabulary\n",
        "                         \n",
        "            continue\n",
        "    if count != 0:\n",
        "        vec /= count\n",
        "    return vec"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMPA_WJaz-VL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wordvec_arrays = np.zeros((len(tokenized_tweet), 200))\n",
        "\n",
        "for i in range(len(tokenized_tweet)):\n",
        "    wordvec_arrays[i,:] = word_vector(tokenized_tweet[i], 200)\n",
        "    \n",
        "wordvec_df = pd.DataFrame(wordvec_arrays)\n",
        "wordvec_df.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nP7KFHM0ED6",
        "colab_type": "text"
      },
      "source": [
        "### Model Building"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQpuzhBzz-ZQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kP-_DOQz-a7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_bow = bow[:31962,:]\n",
        "test_bow = bow[31962:,:]\n",
        "\n",
        "# splitting data into training and validation set\n",
        "xtrain_bow, xvalid_bow, ytrain, yvalid = train_test_split(train_bow, train['label'],  \n",
        "                                                          random_state=42, \n",
        "                                                          test_size=0.3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zp40WHO1z-fA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lreg = LogisticRegression()\n",
        "lreg.fit(xtrain_bow, ytrain) # training the model\n",
        "\n",
        "prediction = lreg.predict_proba(xvalid_bow) # predicting on the validation set\n",
        "prediction_int = prediction[:,1] >= 0.3 # if prediction is greater than or equal to 0.3 than 1 else 0\n",
        "prediction_int = prediction_int.astype(np.int)\n",
        "\n",
        "f1_score(yvalid, prediction_int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TgufxNb-0QlK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_pred = lreg.predict_proba(test_bow)\n",
        "test_pred_int = test_pred[:,1] >= 0.3\n",
        "test_pred_int = test_pred_int.astype(np.int)\n",
        "test['label'] = test_pred_int\n",
        "submission = test[['id','label']]\n",
        "submission.to_csv('sub_lreg_bow.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXrXDaeZ0g59",
        "colab_type": "text"
      },
      "source": [
        "TF-IDF Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvitgYig0Qrf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_tfidf = tfidf[:31962,:]\n",
        "test_tfidf = tfidf[31962:,:]\n",
        "\n",
        "xtrain_tfidf = train_tfidf[ytrain.index]\n",
        "xvalid_tfidf = train_tfidf[yvalid.index]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cW4vdp0F0jxi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lreg.fit(xtrain_tfidf, ytrain)\n",
        "\n",
        "prediction = lreg.predict_proba(xvalid_tfidf)\n",
        "prediction_int = prediction[:,1] >= 0.3\n",
        "prediction_int = prediction_int.astype(np.int)\n",
        "\n",
        "f1_score(yvalid, prediction_int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Egm2wqdn0qzZ",
        "colab_type": "text"
      },
      "source": [
        "Word2Vec Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIcMX3Rd0kC3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_w2v = wordvec_df.iloc[:31962,:]\n",
        "test_w2v = wordvec_df.iloc[31962:,:]\n",
        "\n",
        "xtrain_w2v = train_w2v.iloc[ytrain.index,:]\n",
        "xvalid_w2v = train_w2v.iloc[yvalid.index,:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7VSln6n0kHB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lreg.fit(xtrain_w2v, ytrain)\n",
        "\n",
        "prediction = lreg.predict_proba(xvalid_w2v)\n",
        "prediction_int = prediction[:,1] >= 0.3\n",
        "prediction_int = prediction_int.astype(np.int)\n",
        "f1_score(yvalid, prediction_int)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}